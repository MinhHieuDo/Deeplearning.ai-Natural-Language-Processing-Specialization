# Course2: Natural Language Processing with Probabilistic Models
## Week1: Autocorrect and Dynamic Programming
### Autocorrect
![](Figures/Week1/autocorrect.png)
![](Figures/Week1/Misspelling.png)
![](Figures/Week1/Find_strings.png)
![](Figures/Week1/Filter.png)
![](Figures/Week1/Calc_prob.png)
### Minimum edit distance
![](Figures/Week1/Mini_distance.png)
![](Figures/Week1/Mini_distance_tabular.png)
![](Figures/Week1/Mini_distance_AL.png)
![](Figures/Week1/Mini_distance_full.png)
## Week2: Part of Speech Tagging and Hidden Markov Models
### Markov Chains
![](Figures/Week2/Transition_matrix.png)
![](Figures/Week2/Emission_matrix.png)

![](Figures/Week2/Trainsition_prob.png)
![](Figures/Week2/Populating_trainsition_matrix.png)
![](Figures/Week2/Smoothing.png)
![](Figures/Week2/Populating_emission_matrix.png)
### The Viterbi Algorithm
![](Figures/Week2/Viterbi_alg.png)
### Viterbi: Initialization
![](Figures/Week2/Initialization_step.png)
![](Figures/Week2/Forward.png)
![](Figures/Week2/Backward.png)
## Week3: Autocomplete and Language Models
### N-Grams
![](Figures/Week3/N-gram_def.png)
![](Figures/Week3/N-gram_prob.png)
### Sequence Probabilities
![](Figures/Week3/Prob_sequence.png)
![](Figures/Week3/Problem.png)
![](Figures/Week3/Aprox_sentence_prob.png)
### Starting and Ending Sentences
![](Figures/Week3/Start_of_sentence.png)
![](Figures/Week3/End_of_sentence.png)
### The N-gram Language Model
![](Figures/Week3/Prob_matrix.png)
![](Figures/Week3/Language_model.png)
![](Figures/Week3/Generative_language_model.png)
### Language Model Evaluation
![](Figures/Week3/Perplexity.png)
![](Figures/Week3/Perplexity_bigram.png)
![](Figures/Week3/LogPerplexity.png)
### Out of Vocabulary Words
![](Figures/Week3/Out_of_vocab.png)
### Smoothing
![](Figures/Week3/Smoothing.png)
![](Figures/Week3/Backoff.png)
![](Figures/Week3/Interpolation.png)

## Week4: Word embeddings with neural networks
### Basic Word Representations
![](Figures/Week4/One_hot_vector.png)
### Word Embeddings
![](Figures/Week4/Word_embedding_vector.png)
![](Figures/Week4/Word_embedding_process.png)
![](Figures/Week4/Basic_word_embedding.png)
![](Figures/Week4/Avanced_word_embedding.png)
### Continuous Bag-of-Words Model
![](Figures/Week4/CBOW.png)
### Cleaning and Tokenization
![](Figures/Week4/Preprocess_data.png)
### Sliding Window of Words in Python
![](Figures/Week4/Sliding_window.png)
![](Figures/Week4/Sliding_window_p2.png)
### Transforming Words into Vectors
![](Figures/Week4/Word_into_vector.png)
![](Figures/Week4/Word_into_vector_final.png)
### Architecture of the CBOW Model
![](Figures/Week4/CBOW_model.png)
![](Figures/Week4/CBOW_single_input.png)
![](Figures/Week4/CBOW_batch_input.png)
![](Figures/Week4/CBOW_relu.png)
![](Figures/Week4/CBOW_softmax.png)
### Training a CBOW Model
![](Figures/Week4/Loss_model.png)
![](Figures/Week4/Cross_entropy_loss.png)
![](Figures/Week4/Cost.png)
![](Figures/Week4/Backpropagation.png)
![](Figures/Week4/Gradient_descent.png)
### Extracting Word Embedding Vectors
![](Figures/Week4/Extracting_option1.png)
![](Figures/Week4/Extracting_option2.png)
![](Figures/Week4/Extracting_option3.png)
### Evaluating Word Embeddings: Intrinsic Evaluation
![](Figures/Week4/Intrinsic_evaluation.png)