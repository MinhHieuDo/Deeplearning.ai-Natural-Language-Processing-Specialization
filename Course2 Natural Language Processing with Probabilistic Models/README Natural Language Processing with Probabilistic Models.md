# Course2: Natural Language Processing with Probabilistic Models
## Week1: Autocorrect and Dynamic Programming
### Autocorrect
![](Figures/Week1/autocorrect.png)
![](Figures/Week1/Misspelling.png)
![](Figures/Week1/Find_strings.png)
![](Figures/Week1/Filter.png)
![](Figures/Week1/Calc_prob.png)
### Minimum edit distance
![](Figures/Week1/Mini_distance.png)
![](Figures/Week1/Mini_distance_tabular.png)
![](Figures/Week1/Mini_distance_AL.png)
![](Figures/Week1/Mini_distance_full.png)
## Week2: Part of Speech Tagging and Hidden Markov Models
### Markov Chains
![](Figures/Week2/Transition_matrix.png)
![](Figures/Week2/Emission_matrix.png)

![](Figures/Week2/Trainsition_prob.png)
![](Figures/Week2/Populating_trainsition_matrix.png)
![](Figures/Week2/Smoothing.png)
![](Figures/Week2/Populating_emission_matrix.png)
### The Viterbi Algorithm
![](Figures/Week2/Viterbi_alg.png)
### Viterbi: Initialization
![](Figures/Week2/Initialization_step.png)
![](Figures/Week2/Forward.png)
![](Figures/Week2/Backward.png)