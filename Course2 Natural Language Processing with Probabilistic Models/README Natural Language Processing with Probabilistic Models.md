# Course2: Natural Language Processing with Probabilistic Models
## Week1: Autocorrect and Dynamic Programming
### Autocorrect
![](Figures/Week1/autocorrect.png)
![](Figures/Week1/Misspelling.png)
![](Figures/Week1/Find_strings.png)
![](Figures/Week1/Filter.png)
![](Figures/Week1/Calc_prob.png)
### Minimum edit distance
![](Figures/Week1/Mini_distance.png)
![](Figures/Week1/Mini_distance_tabular.png)
![](Figures/Week1/Mini_distance_AL.png)
![](Figures/Week1/Mini_distance_full.png)
## Week2: Part of Speech Tagging and Hidden Markov Models
### Markov Chains
![](Figures/Week2/Transition_matrix.png)
![](Figures/Week2/Emission_matrix.png)

![](Figures/Week2/Trainsition_prob.png)
![](Figures/Week2/Populating_trainsition_matrix.png)
![](Figures/Week2/Smoothing.png)
![](Figures/Week2/Populating_emission_matrix.png)
### The Viterbi Algorithm
![](Figures/Week2/Viterbi_alg.png)
### Viterbi: Initialization
![](Figures/Week2/Initialization_step.png)
![](Figures/Week2/Forward.png)
![](Figures/Week2/Backward.png)
## Week3: Autocomplete and Language Models
### N-Grams
![](Figures/Week3/N-gram_def.png)
![](Figures/Week3/N-gram_prob.png)
### Sequence Probabilities
![](Figures/Week3/Prob_sequence.png)
![](Figures/Week3/Problem.png)
![](Figures/Week3/Aprox_sentence_prob.png)
### Starting and Ending Sentences
![](Figures/Week3/Start_of_sentence.png)
![](Figures/Week3/End_of_sentence.png)
### The N-gram Language Model
![](Figures/Week3/Prob_matrix.png)
![](Figures/Week3/Language_model.png)
![](Figures/Week3/Generative_language_model.png)
### Language Model Evaluation
![](Figures/Week3/Perplexity.png)
![](Figures/Week3/Perplexity_bigram.png)
![](Figures/Week3/LogPerplexity.png)
### Out of Vocabulary Words
![](Figures/Week3/Out_of_vocab.png)
### Smoothing
![](Figures/Week3/Smoothing.png)
![](Figures/Week3/Backoff.png)
![](Figures/Week3/Interpolation.png)