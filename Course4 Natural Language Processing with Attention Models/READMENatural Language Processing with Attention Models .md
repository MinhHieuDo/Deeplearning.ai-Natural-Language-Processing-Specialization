# Course4: Natural Language Processing with Attention Models
## Week1: Neural Machine Translation
### Seq2seq
![](Figures/Week1/Seq2seq.png)
![](Figures/Week1/Attention.png)
### Attention
![](Figures/Week1/Attention.png)
![](Figures/Week1/Flexible_attention.png)
### Training an NMT with Attention
![](Figures/Week1/NMT.png)
### Evaluation for Machine Translation
![](Figures/Week1/Bleu.png)
![](Figures/Week1/Bleu_score.png)
![](Figures/Week1/Bleu_problem.png)
![](Figures/Week1/Rouge_recall.png)
![](Figures/Week1/Rouge_precision.png)
![](Figures/Week1/Rouge_problem.png)
### Sampling and Decoding
![](Figures/Week1/Greedy_decoding.png)
![](Figures/Week1/Random_sampling.png)
![](Figures/Week1/Temperature.png)
![](Figures/Week1/Beam_search.png)
![](Figures/Week1/Beam_search_ex.png)
![](Figures/Week1/Beam_search_problem.png)
![](Figures/Week1/MBR.png)
![](Figures/Week1/MBR_ex.png)
