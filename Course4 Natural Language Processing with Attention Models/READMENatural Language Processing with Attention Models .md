# Course4: Natural Language Processing with Attention Models
## Week1: Neural Machine Translation
### Seq2seq
![](Figures/Week1/Seq2seq.png)
![](Figures/Week1/Attention.png)
### Attention
![](Figures/Week1/Attention.png)
![](Figures/Week1/Flexible_attention.png)
### Training an NMT with Attention
![](Figures/Week1/NMT.png)
### Evaluation for Machine Translation
![](Figures/Week1/Bleu.png)
![](Figures/Week1/Bleu_score.png)
![](Figures/Week1/Bleu_problem.png)
![](Figures/Week1/Rouge_recall.png)
![](Figures/Week1/Rouge_precision.png)
![](Figures/Week1/Rouge_problem.png)
### Sampling and Decoding
![](Figures/Week1/Greedy_decoding.png)
![](Figures/Week1/Random_sampling.png)
![](Figures/Week1/Temperature.png)
![](Figures/Week1/Beam_search.png)
![](Figures/Week1/Beam_search_ex.png)
![](Figures/Week1/Beam_search_problem.png)
![](Figures/Week1/MBR.png)
![](Figures/Week1/MBR_ex.png)

## Week2: Text Summarization
### Transformers vs RNNs
![](Figures/Week2/Encoder_Decoder.png)
![](Figures/Week2/Encoder_Decoder_Attention.png)
![](Figures/Week2/Multi_headed.png)
### Transformer Applications
![](Figures/Week2/T5_app1.png)
![](Figures/Week2/T5_app2.png)
### Dot-Product Attention
![](Figures/Week2/Intro_attention.png)
![](Figures/Week2/Attention_math.png)
![](Figures/Week2/Attention_formula.png)
### Causal Attention
![](Figures/Week2/Three_ways.png)
![](Figures/Week2/Casual_attention.png)
### Multi-head Attention
![](Figures/Week2/Scale.png)
![](Figures/Week2/Concatenation.png)
![](Figures/Week2/MH_formula.png)

### Transformer Decoder
![](Figures/Week2/Transformer_overview.png)
![](Figures/Week2/Transformer_explanation.png)
![](Figures/Week2/Transformer_decoder.png)
![](Figures/Week2/Feed_forward_layer.png)

### Transformer Summarizer
![](Figures/Week2/Data_processing.png)
![](Figures/Week2/Cost_function.png)
![](Figures/Week2/Model_summarize.png)
