# Course4: Natural Language Processing with Attention Models
## Week1: Neural Machine Translation
### Seq2seq
![](Figures/Week1/Seq2seq.png)
![](Figures/Week1/Attention.png)
### Attention
![](Figures/Week1/Attention.png)
![](Figures/Week1/Flexible_attention.png)
### Training an NMT with Attention
![](Figures/Week1/NMT.png)
### Evaluation for Machine Translation
![](Figures/Week1/Bleu.png)
![](Figures/Week1/Bleu_score.png)
![](Figures/Week1/Bleu_problem.png)
![](Figures/Week1/Rouge_recall.png)
![](Figures/Week1/Rouge_precision.png)
![](Figures/Week1/Rouge_problem.png)
### Sampling and Decoding
![](Figures/Week1/Greedy_decoding.png)
![](Figures/Week1/Random_sampling.png)
![](Figures/Week1/Temperature.png)
![](Figures/Week1/Beam_search.png)
![](Figures/Week1/Beam_search_ex.png)
![](Figures/Week1/Beam_search_problem.png)
![](Figures/Week1/MBR.png)
![](Figures/Week1/MBR_ex.png)

## Week2: Text Summarization
### Transformers vs RNNs
![](Figures/Week2/Encoder_Decoder.png)
![](Figures/Week2/Encoder_Decoder_Attention.png)
![](Figures/Week2/Multi_headed.png)
### Transformer Applications
![](Figures/Week2/T5_app1.png)
![](Figures/Week2/T5_app2.png)
### Dot-Product Attention
![](Figures/Week2/Intro_attention.png)
![](Figures/Week2/Attention_math.png)
![](Figures/Week2/Attention_formula.png)
### Causal Attention
![](Figures/Week2/Three_ways.png)
![](Figures/Week2/Casual_attention.png)
### Multi-head Attention
![](Figures/Week2/Scale.png)
![](Figures/Week2/Concatenation.png)
![](Figures/Week2/MH_formula.png)

### Transformer Decoder
![](Figures/Week2/Transformer_overview.png)
![](Figures/Week2/Transformer_explanation.png)
![](Figures/Week2/Transformer_decoder.png)
![](Figures/Week2/Feed_forward_layer.png)

### Transformer Summarizer
![](Figures/Week2/Data_processing.png)
![](Figures/Week2/Cost_function.png)
![](Figures/Week2/Model_summarize.png)

## Week3: Question Answering
### Transfer Learning in NLP
![](Figures/Week3/Transfer_learning.png)
![](Figures/Week3/Feature_based_Fine_tuning.png)
![](Figures/Week3/Pretrain_task.png)

### ELMo, GPT, BERT, T5
![](Figures/Week3/CBOW.png)
![](Figures/Week3/ELMO.png)
![](Figures/Week3/GPT.png)
![](Figures/Week3/BERT.png)
![](Figures/Week3/T5_overview.png)
![](Figures/Week3/summary.png)

### Bidirectional Encoder Representations from Transformers 
![](Figures/Week3/BERT_layer.png)
![](Figures/Week3/Summary_BERT.png)

### BERT Objective
![](Figures/Week3/BERT_input.png)
![](Figures/Week3/BERT_output.png)
![](Figures/Week3/BERT_loss.png)
### Fine tuning BERT
![](Figures/Week3/BERT_fine.png)
![](Figures/Week3/BERT_fine_in_out.png)
![](Figures/Week3/BERT_fine_summary.png)
### Transformer: T5
![](Figures/Week3/Architecture.png)
![](Figures/Week3/T5_multi_task.png)
![](Figures/Week3/T5_input_output.png)

![](Figures/Week3/T5_fine_tune.png)
![](Figures/Week3/Unfreezing_adapter.png)

### Question Answering
![](Figures/Week3/Transformer_encoder.png)
![](Figures/Week3/QA_T5.png)

## Week4: Chatbot

### Transformer Complexity
![](Figures/Week4/Transformer_issues.png)

![](Figures/Week4/LSH_attention.png)
![](Figures/Week4/LSH_graph.png)

### Reversible Residual Layers 
![](Figures/Week4/Memory.png)
![](Figures/Week4/Reversible_layers.png)
![](Figures/Week4/Reversible1.png)
![](Figures/Week4/Reversible2.png)